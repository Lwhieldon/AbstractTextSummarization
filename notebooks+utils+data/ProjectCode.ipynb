{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"TextSummarizationImg.jpg\" alt=\"TextSummarization\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Text summarization is a complex task for recurrent neural networks, particularly in neural language models. Despite it's complexity, text summarization offers the prospect for domain experts to significantly increase productivity and is used in enterprise-level capacities today to condense common domain knowledge, summarize complex corpus of text like contracts, and automatically generate content for use cases in social media, advertising, and more. In this project, I explore the use of large lanugage models in the recurrent neural network framework using encoder-decoder transformers from scratch to condense dialogues between several people into a crisp summary, demonstrating abstract text summarization. Applications of this exercise are endless, but could be especially beneficial for summarizing long transcripts from meetings and so on.\n",
    "\n",
    "Let's first look at the dataset we will use for training: Samsung transcript data. We will then go into the scoring parameters and demonstrate how we train the model. Lastly, we will then showcase our model's inference and discuss opportunities for future work and study use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & EDA\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For our application, we'll use the <a href=\"https://arxiv.org/abs/1911.12237\">SAMsum dataset</a>, developed by Samsung, which consists of a collection of dialogues along with brief summaries. In an enterprise setting, these dialogues might represent the interactions between a customer and a support center personnel or a transcript representing individuals taking part in a meeting, so generating accurate summaries can help improve customer service, cut down on note taking, and detect common patterns among customer requests or meeting themes. \n",
    "\n",
    "For this project, we leverage ðŸ¤— <a href=\"https://huggingface.co/datasets/samsum\">Hugging Face's SAMsum dataset</a> by leveraging the `load_dataset` library. This is beneficial as Hugging Face has already performed the task of cleansing and organizing the SAMsum dataset for us. \n",
    "\n",
    "Let's load it, look at is features, and review an example transcript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset samsum (C:\\Users\\lwhieldon\\.cache\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc2c1e544e3456a86b23844d9567981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['id', 'dialogue', 'summary']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "The dataset has 3 columns: \n",
    " - __dialogue__, which contains the dialogue text, \n",
    " - __summary__ containing the synopsis of the dialogue, and \n",
    " - __id__ to uniquely identify each record. \n",
    " \n",
    " Let's explore both the features available in ðŸ¤— Hugging Face's dataset and look at an excerpt from a dialogue record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "sample_text = dataset_samsum[\"train\"][1][\"dialogue\"][:2000]\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is made of 16,369 conversations distributed uniformly into 4 groups based on the number of utterances in conversations: 3-6, 7-12, 13-18, and 19-30. Each utterance contains the names of the speaker. Note also the data is split into the following subsets:\n",
    "\n",
    "__Data Splits__\n",
    "- train: 14,732 records\n",
    "- validation: 818 records\n",
    "- test: 819 records\n",
    "\n",
    "The dialogues look like what you would expect from a chat via SMS or WhatsApp, including emojis and placeholders for GIFs. The dialogue field contains the full text and the summary the summarized dialogue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Techniques\n",
    "\n",
    "It is common to use a baseline model first and then train a model on to, demonstrating transfer learning techniques. We use PEGASUS trained on the CNN/DailyMail dataset as our baseline summarization pipeline. We also need to replace special tokens in the dialogue so that the model can understand newlines as part of the body of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
      "Hannah: I'd rather you texted him.\n",
      "Amanda: Just text him .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "pipe = pipeline(\"summarization\",model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"Summary: \")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\",\".\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model mostly tries to summarize by extracting the key sentences from the dialogue. This probably worked relatively well on the CNN/DailyMail dataset, but the summaries in SAMSum are more abstract. Let's explore important scoring metrics and then evaluate the full ROUGE evaluation on the test set of the SAMsum dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Metrics\n",
    "\n",
    "Good evaluation metrics are important, since we use them to measure the performance of models not only when we train them but also later when we deploy them to production. If we have bad metrics, we may not have enough visibility into the model's performance degradation overtime. Most importantly, the model may then shift from the intended business objectives and goals which lessen its value proposition. \n",
    "\n",
    "It's comparatively not as easy to measure performance on text generation tasks as with classification tasks like sentiment analysis or entity-name recognition. Simply checking for an optimal or exact match to a reference point is not sufficient. Because of this challenge, some alternative scoring metrics were created. In this project, we use the ROUGE metric for our scoring parameters.\n",
    "\n",
    "## ROUGE Metric\n",
    "\n",
    "The ROUGE score was specifically developed for applications like summarization where high recall is more important than just precision. This approach looks at different n-grams, or a sequence of n-words, and compare their occurrences in the generated text and the reference texts. ROUGE checks how many n-grams in the reference text also occur in the generated text. The precision formula is useful for this as we count the (unclipped) occurrence of reference n-grams in the generated text in the denominator:\n",
    "\n",
    "$$ROUGE-N = \\frac{\\sum_{ snt' \\in C}\\sum_{n-gram\\in snt'}Count_{match}(n-gram)}{\\sum_{snt'\\in C}\\sum_{n-gram\\in snt'}Count(n-gram)}$$\n",
    "\n",
    "This was the original proposal for ROUGE. Subsequently, researchers have found that fully removing precision can have strong negative effects. We can measure precision well, and we can then combine both precision and recall ROUGE scores in the harmonic mean to get an $F^1$-score. This score is the metric that is nowadays commonly reported for ROUGE.\n",
    "\n",
    "There is a separate score in ROUGE to measure the __longest common substring__ (LCS), called ROUGE-L. The LCS can be calculated for any pair of strings. For example, the LCS for \"abab\" and \"abc\" would be \"ab\", and its length would be 2. If we want to compare this value between two samples we need to somehow normalize it because otherwise a longer text would be at an advantage. To achieve this, the inventors of ROUGE came up with an F-Score-like scheme where the LCS is normalized with the length of the reference and generated text, then the two normalized scores are mixed together\n",
    "\n",
    "$$R_{LCS} = \\frac{LCS(X,Y)}{m}$$\n",
    "\n",
    "$$P_{LCS} = \\frac{LCS(X,Y)}{n}$$\n",
    "\n",
    "$$F_{LCS} = \\frac{(1 + \\beta^2) R_{LCS}P_{LCS}}{R_{LCS} + \\beta P_{LCS}}, where \\beta = P_{LCS}/R_{LCS}$$\n",
    "\n",
    "The way the LCS score is properly normalized and can be compared across samples. In ðŸ¤— Hugging Face's Dataset implementations, two variations of ROUGE are calculated: one that calculates the score per sentence and averages it for the summaries (ROUGE-L), and the other that calculates it directly over the whole summary (ROUGE-Lsum).\n",
    "\n",
    "We can load the metric as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge_score in c:\\users\\lwhieldon\\appdata\\roaming\\python\\python39\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\lwhieldon\\appdata\\roaming\\python\\python39\\site-packages (from rouge_score) (1.3.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from rouge_score) (1.21.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk->rouge_score) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score\n",
    "from datasets import load_metric\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import AttemptManager\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements),batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset,\n",
    "                                metric,\n",
    "                                model,\n",
    "                                tokenizer,\n",
    "                                batch_size=16,\n",
    "                                device=device,\n",
    "                                column_text = \"dialogue\",\n",
    "                                column_summary = \"summary\"):\n",
    "    dialogue_batches = list(chunks(dataset[column_text],batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary],batch_size))\n",
    "    \n",
    "    for dialogue_batch, target_batch in tqdm(zip(dialogue_batches, target_batches), total=len(dialogue_batches)):\n",
    "        inputs = tokenizer(dialogue_batch, max_length=1024, truncation=True, padding=\"max_length\",return_tensors=\"pt\")\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,clean_up_tokenization_space=True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\",\" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    score = metric.compute()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "\n",
    "This section highlights the model we have elected to train SAMsum data. We first explore important concepts of the model and then we evaluate the model using a baseline before we begin fine-tuning the model on the SAMsum dataset.\n",
    "\n",
    "### PEGASUS\n",
    "\n",
    "PEGASUS is an encoder-decoder transformer. As shown in figure below, its pretraining objective is to predict masked sentences in multisentence texts. The authors of [insert PEGASUS authors here] argue that the closer the pretraining objective is to the downstream task, the more effective it is. With the aim of finding a pretraining objective that is closer to summarization than general lanugage modeling, they automatically identified, in a very large corpus, sentences containing most of the content of their surrounding paragraphs (using summarization evaluation metrics as a heuristic for content overlap) and pretrained the PEGASUS model to reconstruct these sentences, thereby obtaining a state-of-the-art model for text summarization.\n",
    "\n",
    "![PegasusArchitecture](PegasusArchitecture.png)\n",
    "\n",
    "This model has a special token for newlines, which is why we don't need the sent_tokenize() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the scoring metrics, we create a simple list that we can visualize the evaluation criteria against in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = dataset_samsum[\"train\"][1][\"summary\"]\n",
    "rouge_names = [\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to conduct the full ROUGE evaluation on the test set of the SAMsum dataset before training the model as our baseline. The output of the scoring is displayed in a pandas dataframe for easier visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [7:33:38<00:00, 264.26s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.29614</td>\n",
       "      <td>0.087609</td>\n",
       "      <td>0.229381</td>\n",
       "      <td>0.229379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rouge1    rouge2    rougeL  rougeLsum\n",
       "pegasus  0.29614  0.087609  0.229381   0.229379"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"],\n",
    "                                    rouge_metric, \n",
    "                                    model, \n",
    "                                    tokenizer, \n",
    "                                    column_text=\"dialogue\",\n",
    "                                    column_summary=\"summary\",\n",
    "                                    batch_size=8)\n",
    "rouge_dict = dict((rn,score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results aren't great but this is not unexpected since the training objective in the CNN/Daily mail dataset is summarizing an article versus transcript dialogue. Nevertheless, setting up the evaluation pipeline before training has two advantages: we can directy measure the success of training with the metric and we have a good baseline. Fine-tuning the model on our dataset should result in an immediate improvement in the ROUGE metric, and if that is not the case we'll know something is wrong with our training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning PEGASUS\n",
    "\n",
    "Before we process the data for training, let's have a quick look at the length distribution of the input and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x252 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n",
    "\n",
    "fig,axes = plt.subplots(1,2,figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\",edgecolor = \"C0\")\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[1].hist(s_len,bins=20,color=\"C0\",edgecolor=\"C0\")\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to look at the length distribution of the dialogue (input) and summaries (output) before we train our model. We see that most dialogues are fairly short, containing around 100-200 tokens (a.k.a words) per dialogue. Similarly, summaries are shorter, with around 20-40 tokens (which you can compare to the average length of a tweet). \n",
    "\n",
    "Let's keep the observations from the EDA section in mind and build the data collator for the Trainer. First we need to tokenize the dataset. For now, we'll set the maximum lengths to 1024 and 128 for the dialogues and summaries, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\lwhieldon\\.cache\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\\cache-be6c73ebb8f747e0.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf4033a9058402aac13bbf661e6983a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwhieldon\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\lwhieldon\\.cache\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\\cache-737c02a803f2c8ef.arrow\n"
     ]
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"],max_length=1024, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,truncation=True)\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)\n",
    "\n",
    "columns = [\"input_ids\",\"labels\",\"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\",columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new thing in the use of the tokenization step is the `tokenizer.as_target_tokenizer()` context. Some models require special tokens in the decoder inputs, so it's important to differentiate between the tokenization of encoder and decoder iputs. In the `with` statement (called a _context manager_), the tokenizer knows that it is tokenizing for the decoder and can process sequences accordingly.\n",
    "\n",
    "Now we need to create the data collator. This function is called in the `Trainer` just before the batch is fed through the model. In most cases we can use the default collator, which collects all the tensors from the batch and simply stacks them. For the summarization task we need to not only stack the inputs but also prepare the targets on the decoder side. PEGASUS is an encoder-decoder transformer and thus has the classic seq2seq architecture. In a seq2seq setup, a common approach is to apply \"teacher forcing\" in the decoder. With this strategy, the decoder receives input tokens (like in decoder-only models such as GPT-2) that consists of the labels shifted by one in additiona to the encoder output; so, when making the prediction for the next token the decoder gets the ground truth shifted by one as an input.\n",
    "\n",
    "We shift it by one so that the decoder only see the previous group truth labels and not the current or future ones. Shifting alone suffices since the decoder has masked self-attention that masks all inputs at present and in the future.\n",
    "\n",
    "So, when we prepare our batch, we set up the decoder inputs by shifting the labels to the right by one. After that, we make sure the padding tokens in the labels are ignored by the loss function by setting them to -100. We actually don't have to do this manually, though, since the `DataCollatorForSeq2Seq` comes to the rescue and takes care of all these steps for us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as usual, we set up the `TrainingArguments` for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir='pegasus-samsum',\n",
    "                                  num_train_epochs=1,\n",
    "                                  warmup_steps=500,\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_steps=10,\n",
    "                                  push_to_hub=True,\n",
    "                                  evaluation_strategy='steps',\n",
    "                                  eval_steps=500,\n",
    "                                  save_steps=1e6,\n",
    "                                  gradient_accumulation_steps=16\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that is different from the previous settings is that new argument, `gradient_accumulation_steps`. Since the model is quite big, we had to set the batch size to 1. However, a batch size that is too small can hurt convergence. To resolve the issue, we can use a nifty technique called _gradient accummulation_. As the name suggests, instead of calculating the gradients of the full batch all at once, we make smaller batches and aggregate the gradients. When we have aggregated enough gradients, we run the optimization step. Naturally, this is a bit slower than doing it in one pass, but it saves us a lot of GPU memory.\n",
    "\n",
    "Let's now make sure that we are logged in to Hugging Face so we can push the model to the Hub after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to C:\\Users\\lwhieldon/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to initialize the trainer with the model, tokenizer, training arguments, and data collator, as well as the training and evaluation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/Lwhieldon/pegasus-samsum into local empty directory.\n",
      "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/Lwhieldon/pegasus-samsum into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92186d95d32e4063ab08ecdbb0356608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 8.00k/2.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea12428f99444d0943dc8d8f5899dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 3.23k/3.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3cb85f56d44af18c695f8a8a233011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file spiece.model:   1%|1         | 24.0k/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8632268854c844a38e4da32c650df5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file spiece.model:   0%|          | 1.00k/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9804b6023db4424aa761481f9176db96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  31%|###       | 1.00k/3.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bbfc9ef43e4f65835005b1e167cfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/2.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready for training. After training, we can directly run the evaluation function on the test set to see how well the model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: summary, dialogue, id. If summary, dialogue, id are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\lwhieldon\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8333a9e21cd54d20b0bc5ce9f4e7c8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3369, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 2.8633, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}\n",
      "{'loss': 3.0138, 'learning_rate': 3e-06, 'epoch': 0.03}\n",
      "{'loss': 2.901, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 2.9721, 'learning_rate': 5e-06, 'epoch': 0.05}\n",
      "{'loss': 2.7123, 'learning_rate': 6e-06, 'epoch': 0.07}\n",
      "{'loss': 2.6676, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.08}\n",
      "{'loss': 2.5026, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 2.5378, 'learning_rate': 9e-06, 'epoch': 0.1}\n",
      "{'loss': 2.2458, 'learning_rate': 1e-05, 'epoch': 0.11}\n",
      "{'loss': 2.2491, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 2.2101, 'learning_rate': 1.2e-05, 'epoch': 0.13}\n",
      "{'loss': 2.0892, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 2.0286, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 2.1549, 'learning_rate': 1.5e-05, 'epoch': 0.16}\n",
      "{'loss': 2.0327, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 1.9865, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.18}\n",
      "{'loss': 1.9692, 'learning_rate': 1.8e-05, 'epoch': 0.2}\n",
      "{'loss': 1.9704, 'learning_rate': 1.9e-05, 'epoch': 0.21}\n",
      "{'loss': 1.8508, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 1.9515, 'learning_rate': 2.1e-05, 'epoch': 0.23}\n",
      "{'loss': 1.9415, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.24}\n",
      "{'loss': 1.8463, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.25}\n",
      "{'loss': 1.7896, 'learning_rate': 2.4e-05, 'epoch': 0.26}\n",
      "{'loss': 1.7689, 'learning_rate': 2.5e-05, 'epoch': 0.27}\n",
      "{'loss': 1.8888, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.28}\n",
      "{'loss': 1.7603, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.29}\n",
      "{'loss': 1.7568, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.3}\n",
      "{'loss': 1.7939, 'learning_rate': 2.9e-05, 'epoch': 0.31}\n",
      "{'loss': 1.7068, 'learning_rate': 3e-05, 'epoch': 0.33}\n",
      "{'loss': 1.8142, 'learning_rate': 3.1e-05, 'epoch': 0.34}\n",
      "{'loss': 1.6243, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.35}\n",
      "{'loss': 1.8189, 'learning_rate': 3.3e-05, 'epoch': 0.36}\n",
      "{'loss': 1.7189, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.37}\n",
      "{'loss': 1.7975, 'learning_rate': 3.5e-05, 'epoch': 0.38}\n",
      "{'loss': 1.706, 'learning_rate': 3.6e-05, 'epoch': 0.39}\n",
      "{'loss': 1.7436, 'learning_rate': 3.7e-05, 'epoch': 0.4}\n",
      "{'loss': 1.7208, 'learning_rate': 3.8e-05, 'epoch': 0.41}\n",
      "{'loss': 1.7915, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.42}\n",
      "{'loss': 1.6716, 'learning_rate': 4e-05, 'epoch': 0.43}\n",
      "{'loss': 1.6948, 'learning_rate': 4.1e-05, 'epoch': 0.45}\n",
      "{'loss': 1.6841, 'learning_rate': 4.2e-05, 'epoch': 0.46}\n",
      "{'loss': 1.6064, 'learning_rate': 4.3e-05, 'epoch': 0.47}\n",
      "{'loss': 1.7886, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.48}\n",
      "{'loss': 1.6181, 'learning_rate': 4.5e-05, 'epoch': 0.49}\n",
      "{'loss': 1.7532, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.5}\n",
      "{'loss': 1.5376, 'learning_rate': 4.7e-05, 'epoch': 0.51}\n",
      "{'loss': 1.6767, 'learning_rate': 4.8e-05, 'epoch': 0.52}\n",
      "{'loss': 1.6208, 'learning_rate': 4.9e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: summary, dialogue, id. If summary, dialogue, id are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6776, 'learning_rate': 5e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2fed8a9ed74efc8aedab4c59fbf606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4919248819351196, 'eval_runtime': 644.959, 'eval_samples_per_second': 1.268, 'eval_steps_per_second': 1.268, 'epoch': 0.54}\n",
      "{'loss': 1.7402, 'learning_rate': 4.880952380952381e-05, 'epoch': 0.55}\n",
      "{'loss': 1.6463, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.56}\n",
      "{'loss': 1.6937, 'learning_rate': 4.642857142857143e-05, 'epoch': 0.58}\n",
      "{'loss': 1.6954, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.59}\n",
      "{'loss': 1.6894, 'learning_rate': 4.404761904761905e-05, 'epoch': 0.6}\n",
      "{'loss': 1.6045, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.61}\n",
      "{'loss': 1.6103, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.62}\n",
      "{'loss': 1.5907, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.63}\n",
      "{'loss': 1.6141, 'learning_rate': 3.928571428571429e-05, 'epoch': 0.64}\n",
      "{'loss': 1.5658, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.65}\n",
      "{'loss': 1.5725, 'learning_rate': 3.690476190476191e-05, 'epoch': 0.66}\n",
      "{'loss': 1.5356, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.67}\n",
      "{'loss': 1.5834, 'learning_rate': 3.4523809523809526e-05, 'epoch': 0.68}\n",
      "{'loss': 1.5951, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.7}\n",
      "{'loss': 1.6171, 'learning_rate': 3.2142857142857144e-05, 'epoch': 0.71}\n",
      "{'loss': 1.6349, 'learning_rate': 3.095238095238095e-05, 'epoch': 0.72}\n",
      "{'loss': 1.5907, 'learning_rate': 2.9761904761904762e-05, 'epoch': 0.73}\n",
      "{'loss': 1.5537, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.74}\n",
      "{'loss': 1.6543, 'learning_rate': 2.7380952380952383e-05, 'epoch': 0.75}\n",
      "{'loss': 1.5864, 'learning_rate': 2.6190476190476192e-05, 'epoch': 0.76}\n",
      "{'loss': 1.62, 'learning_rate': 2.5e-05, 'epoch': 0.77}\n",
      "{'loss': 1.5453, 'learning_rate': 2.380952380952381e-05, 'epoch': 0.78}\n",
      "{'loss': 1.6143, 'learning_rate': 2.261904761904762e-05, 'epoch': 0.79}\n",
      "{'loss': 1.5883, 'learning_rate': 2.1428571428571428e-05, 'epoch': 0.8}\n",
      "{'loss': 1.6057, 'learning_rate': 2.023809523809524e-05, 'epoch': 0.81}\n",
      "{'loss': 1.5434, 'learning_rate': 1.9047619047619046e-05, 'epoch': 0.83}\n",
      "{'loss': 1.5629, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.84}\n",
      "{'loss': 1.5775, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.85}\n",
      "{'loss': 1.5233, 'learning_rate': 1.5476190476190476e-05, 'epoch': 0.86}\n",
      "{'loss': 1.6225, 'learning_rate': 1.4285714285714285e-05, 'epoch': 0.87}\n",
      "{'loss': 1.5946, 'learning_rate': 1.3095238095238096e-05, 'epoch': 0.88}\n",
      "{'loss': 1.6265, 'learning_rate': 1.1904761904761905e-05, 'epoch': 0.89}\n",
      "{'loss': 1.5706, 'learning_rate': 1.0714285714285714e-05, 'epoch': 0.9}\n",
      "{'loss': 1.5927, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.91}\n",
      "{'loss': 1.5898, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.92}\n",
      "{'loss': 1.6879, 'learning_rate': 7.142857142857143e-06, 'epoch': 0.93}\n",
      "{'loss': 1.5291, 'learning_rate': 5.9523809523809525e-06, 'epoch': 0.94}\n",
      "{'loss': 1.5413, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.96}\n",
      "{'loss': 1.5404, 'learning_rate': 3.5714285714285714e-06, 'epoch': 0.97}\n",
      "{'loss': 1.5248, 'learning_rate': 2.3809523809523808e-06, 'epoch': 0.98}\n",
      "{'loss': 1.5471, 'learning_rate': 1.1904761904761904e-06, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5909, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 49392.0625, 'train_samples_per_second': 0.298, 'train_steps_per_second': 0.019, 'train_loss': 1.8225779782170834, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [4:47:40<00:00, 42.10s/it]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.431695</td>\n",
       "      <td>0.201628</td>\n",
       "      <td>0.346877</td>\n",
       "      <td>0.347153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1    rouge2    rougeL  rougeLsum\n",
       "pegasus  0.431695  0.201628  0.346877   0.347153"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"],\n",
    "                                    rouge_metric,\n",
    "                                    trainer.model,\n",
    "                                    tokenizer,\n",
    "                                    batch_size=2,\n",
    "                                    column_text=\"dialogue\",\n",
    "                                    column_summary=\"summary\")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[f\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ROUGE scores improved considerably over the model without fine-tuning, so even though the previous model was also trained for summarization, it was not well adapted for the new domain. Let's push our model to the Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to pegasus-samsum\n",
      "Configuration saved in pegasus-samsum\\config.json\n",
      "Model weights saved in pegasus-samsum\\pytorch_model.bin\n",
      "tokenizer config file saved in pegasus-samsum\\tokenizer_config.json\n",
      "Special tokens file saved in pegasus-samsum\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c4500aaf2543149ae94bb2ed35f073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 3.23k/3.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/Lwhieldon/pegasus-samsum\n",
      "   077cda2..b283db9  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/Lwhieldon/pegasus-samsum\n",
      "   077cda2..b283db9  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'dataset': {'name': 'samsum', 'type': 'samsum', 'config': 'samsum', 'split': 'train', 'args': 'samsum'}}\n",
      "To https://huggingface.co/Lwhieldon/pegasus-samsum\n",
      "   b283db9..bb71d4a  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:To https://huggingface.co/Lwhieldon/pegasus-samsum\n",
      "   b283db9..bb71d4a  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Lwhieldon/pegasus-samsum/commit/b283db9bfedc624b8a0e54f250c20fe57e86a1c1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "### Generating Dialogue Summaries\n",
    "\n",
    "Looking at the losses and ROUGE score, it seems the model is showing a significant improvement over the original model trained on CNN/DailyMail only. Let's see what a summary generated on a sample from the test set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc872348036403ea21371c56fe56b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\lwhieldon/.cache\\huggingface\\hub\\models--Lwhieldon--pegasus-samsum\\snapshots\\bb71d4a7aa1920d622d76980b3cf017d0f17eaf4\\config.json\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"Lwhieldon/pegasus-samsum\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 32,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\lwhieldon/.cache\\huggingface\\hub\\models--Lwhieldon--pegasus-samsum\\snapshots\\bb71d4a7aa1920d622d76980b3cf017d0f17eaf4\\config.json\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"Lwhieldon/pegasus-samsum\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 32,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a390bdfd56694c2abd002da820672965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\lwhieldon/.cache\\huggingface\\hub\\models--Lwhieldon--pegasus-samsum\\snapshots\\bb71d4a7aa1920d622d76980b3cf017d0f17eaf4\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n",
      "\n",
      "All the weights of PegasusForConditionalGeneration were initialized from the model checkpoint at Lwhieldon/pegasus-samsum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PegasusForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bad8049a3846a7a752286382bf8f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a6b9bf8566421c8756697119df97e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052ea970b7f7446cbbd5ee42cad46c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdda0c993224870a3d3053951c36c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\lwhieldon/.cache\\huggingface\\hub\\models--Lwhieldon--pegasus-samsum\\snapshots\\bb71d4a7aa1920d622d76980b3cf017d0f17eaf4\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\lwhieldon/.cache\\huggingface\\hub\\models--Lwhieldon--pegasus-samsum\\snapshots\\bb71d4a7aa1920d622d76980b3cf017d0f17eaf4\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\lwhieldon/.cache\\huggingface\\hub\\models--Lwhieldon--pegasus-samsum\\snapshots\\bb71d4a7aa1920d622d76980b3cf017d0f17eaf4\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\lwhieldon/.cache\\huggingface\\hub\\models--Lwhieldon--pegasus-samsum\\snapshots\\bb71d4a7aa1920d622d76980b3cf017d0f17eaf4\\tokenizer_config.json\n",
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Reference Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "Amanda can't find Betty's number. Larry called Betty's last time they were at the park together. Hannah wants Amanda to text him.\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\"length_penalty\":0.8,\"num_beams\":8, \"max_length\":128}\n",
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "pipe = pipeline(\"summarization\",model=\"Lwhieldon/pegasus-samsum\",use_auth_token=True)\n",
    "\n",
    "\n",
    "print(\"Dialogue\")\n",
    "print(sample_text)\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "print(pipe(sample_text,**gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much more like the reference summary. It seems the model has learned to synthesize the dialogue into a summary without just extracting passages. Now, the ultimate test: how well does the model work on custom input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue\n",
      " Lee: Hi guys, have you heard of our great professor, Zeanique Barber? \n",
      " William: Yes, I was excited that she became our instructor for UMBC's DATA606 Capstone for Data Science in Fall 2022! \n",
      " Lee: Indeed, she has amazing experience in Data Transformation for organizations and I'm excited to learn from her! \n",
      " William: I know, I too have experience in Data Transformation in cloud technology ;) \n",
      " Lee: Cool, maybe we should do a project about cloud data? What do you think? \n",
      " William: Great idea, how hard can it be?! \n",
      " William: I am in! \n",
      " Lee: Awesome, let's do it together! \n",
      "\n",
      "Inference Summary:\n",
      "Zeanique Barber will be the instructor for the DATA606 Capstone for Data Science in Fall 2022 at UMBC. Lee and William want to do a project about cloud data.\n"
     ]
    }
   ],
   "source": [
    "custom_dialogue = \"\"\"\\\n",
    " Lee: Hi guys, have you heard of our great professor, Zeanique Barber? \n",
    " William: Yes, I was excited that she became our instructor for UMBC's DATA606 Capstone for Data Science in Fall 2022! \n",
    " Lee: Indeed, she has amazing experience in Data Transformation for organizations and I'm excited to learn from her! \n",
    " William: I know, I too have experience in Data Transformation in cloud technology ;) \n",
    " Lee: Cool, maybe we should do a project about cloud data? What do you think? \n",
    " William: Great idea, how hard can it be?! \n",
    " William: I am in! \n",
    " Lee: Awesome, let's do it together! \n",
    "\"\"\"\n",
    "print(\"Dialogue\")\n",
    "print(custom_dialogue)\n",
    "print(\"Inference Summary:\")\n",
    "print(pipe(custom_dialogue,**gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated summary of the custom dialogue makes sense. It summarizes well that all the people in the discussion want to write the book together and does not simply extract single sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions & Limitations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
